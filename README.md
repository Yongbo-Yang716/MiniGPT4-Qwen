MiniGPT4-Qwen is a modular Vision-Language Model (VLM) that integrates:

- EVA-CLIP Vision Encoder
- Q-Former for visual token compression
- Qwen-Chat as the language backbone
- Optional LoRA-based fine-tuning

Built on top of the LAVIS framework, this project focuses on clean architecture, modular design, and research-friendly extensibility.

---

## ðŸ§  Architecture Overview

Image  
â†’ Vision Encoder (ViT)  
â†’ Visual Patch Tokens  
â†’ Q-Former Cross-Attention  
â†’ Query Tokens  
â†’ Linear Projection  
â†’ Qwen-Chat (LLM)  
â†’ Text Generation  

---

## âœ¨ Features

- Fully modular Vision-Language pipeline
- Config-driven training via YAML
- Single-GPU friendly
- Optional DeepSpeed pipeline parallelism
- LoRA integration support
- ChatML format compatibility

---
